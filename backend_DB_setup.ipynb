{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fffef45-c5ad-48b4-9067-9937cfc6ddff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install psycopg\n",
    "#!pip install psycopg2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f681b1-3980-4ef0-b1af-91ed848adf82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n",
      "Connected to the PostgreSQL database...\n"
     ]
    }
   ],
   "source": [
    "import psycopg2, os\n",
    "\n",
    "print('Connecting to the PostgreSQL database...')\n",
    "conn = psycopg2.connect(\n",
    "    host=\"\",\n",
    "    port='',\n",
    "    dbname=\"\",\n",
    "    user=\"\",\n",
    "    password=\"\")\n",
    "print('Connected to the PostgreSQL database...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f70b6a6-6717-4b38-8bc5-e1187157e74e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<connection object at 0x00000231CA8E2DF0; dsn: 'user=postgres password=xxx dbname=APAN5400 host=localhost port=5432', closed: 0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca7786c-4b1c-4a50-bd4b-ace1a26bad9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cursor object at 0x00000231CA9065E0; closed: 0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a cursor\n",
    "cur = conn.cursor()\n",
    "cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e41720-0c68-4c24-9aab-eb707702eab4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL database version:\n",
      "('PostgreSQL 16.2 (Debian 16.2-1.pgdg120+2) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "#execute a statement to make sure the cursor works and hence connected to the database\n",
    "\n",
    "print('PostgreSQL database version:')\n",
    "cur.execute('SELECT version()')\n",
    "\n",
    "# display the PostgreSQL database server version\n",
    "db_version = cur.fetchone()\n",
    "print(db_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51904f42-8d01-46d5-9682-46726a569548",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database table created ...\n"
     ]
    }
   ],
   "source": [
    "#Creating the Neighborhoods table\n",
    "\n",
    "createCmd = \"\"\" CREATE TABLE Neighborhoods (\n",
    "Id SERIAL PRIMARY KEY,\n",
    "State VARCHAR(255),\n",
    "Neighborhood_group VARCHAR(255),\n",
    "Neighborhood VARCHAR(255)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(createCmd)\n",
    "conn.commit()\n",
    "\n",
    "print(\"Database table created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0def5565-8f05-4a14-b8ce-992794aacb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DELETING TABLE!!!!!!\n",
    "#cur.execute(\"DROP TABLE Neighborhoods CASCADE\")\n",
    "#conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6dc7f32-a92b-497c-a751-a3b1d5bfa666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cur.execute(\"DROP TABLE Hosts CASCADE\")\n",
    "#conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4d8691e-305a-458a-bb53-511b6fa24abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database table created ...\n"
     ]
    }
   ],
   "source": [
    "#Creating Hosts Table\n",
    "\n",
    "createCmd = \"\"\" CREATE TABLE Hosts (\n",
    "    Id SERIAL PRIMARY KEY,\n",
    "    Host_url VARCHAR(255),\n",
    "    Name VARCHAR(255),\n",
    "    Since DATE,\n",
    "    Location VARCHAR(255),\n",
    "    About TEXT,\n",
    "    Response_time VARCHAR(50),\n",
    "    Response_rate DOUBLE PRECISION,\n",
    "    Acceptance_rate DOUBLE PRECISION,\n",
    "    Is_superhost BOOLEAN,\n",
    "    Neighborhood VARCHAR(255),\n",
    "    Listings_count INTEGER,\n",
    "    Total_listings_count INTEGER,\n",
    "    Verifications TEXT,\n",
    "    Has_profile_pic BOOLEAN,\n",
    "    Identity_verified BOOLEAN\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(createCmd)\n",
    "conn.commit()\n",
    "\n",
    "print(\"Database table created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ae64f8-9a0f-4df9-817d-40daa3eb860c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database table created ...\n"
     ]
    }
   ],
   "source": [
    "#Creating Listings Table\n",
    "\n",
    "createCmd = \"\"\" CREATE TABLE Listings(\n",
    "    Id BIGINT PRIMARY KEY,\n",
    "    Name VARCHAR(255),\n",
    "    Host_id INTEGER REFERENCES Hosts(Id),\n",
    "    Neighborhood_id INTEGER REFERENCES Neighborhoods(Id),\n",
    "    Description TEXT,\n",
    "    Neighborhood_overview TEXT,\n",
    "    Picture_url VARCHAR(4096),\n",
    "    Latitude DOUBLE PRECISION,\n",
    "    Longitude DOUBLE PRECISION,\n",
    "    Property_type VARCHAR(50),\n",
    "    Accommodates INTEGER,\n",
    "    Bathrooms_text TEXT,\n",
    "    Beds INTEGER,\n",
    "    Bedrooms INTEGER,\n",
    "    Amenities VARCHAR(4096),\n",
    "    Price DOUBLE PRECISION,\n",
    "    Minimum_nights INTEGER,\n",
    "    Maximum_nights INTEGER\n",
    ");\n",
    "            \"\"\"\n",
    "\n",
    "cur.execute(createCmd)\n",
    "conn.commit()\n",
    "\n",
    "print(\"Database table created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84173277-a9fd-415a-983b-6df20da59b62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database table created ...\n"
     ]
    }
   ],
   "source": [
    "#Creating Reviews Table\n",
    "\n",
    "createCmd = \"\"\"CREATE TABLE Reviews (\n",
    "    Id BIGINT,\n",
    "    Listing_id BIGINT REFERENCES Listings(Id),\n",
    "    Date DATE,\n",
    "    Reviewer_id BIGINT,\n",
    "    Reviewer_name VARCHAR(255),\n",
    "    Comments TEXT,\n",
    "    PRIMARY KEY (Id, Listing_id)\n",
    ");\n",
    "            \"\"\"\n",
    "\n",
    "cur.execute(createCmd)\n",
    "conn.commit()\n",
    "\n",
    "print(\"Database table created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6120ffb1-a6bc-40eb-9777-04ad8d4915fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database table created ...\n"
     ]
    }
   ],
   "source": [
    "#Creating Calendar Table\n",
    "\n",
    "createCmd = \"\"\" CREATE TABLE Calendar (\n",
    "    Id SERIAL PRIMARY KEY,\n",
    "    Listing_id BIGINT REFERENCES Listings(Id),\n",
    "    Date DATE,\n",
    "    Available BOOLEAN,\n",
    "    Price DECIMAL(10, 2),\n",
    "    Adjusted_Price DECIMAL(10, 2)\n",
    ");\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "cur.execute(createCmd)\n",
    "conn.commit()\n",
    "\n",
    "print(\"Database table created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21879466-075f-4079-98dd-bdb7965c2838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Processing\n",
    "import re\n",
    "\n",
    "def process_rate(rate_str):\n",
    "    # Check if the rate is \"N/A\" and return None\n",
    "    if rate_str == \"N/A\":\n",
    "        return None\n",
    "    \n",
    "    # If the rate is a percentage, convert to decimal\n",
    "    try:\n",
    "        if rate_str.endswith('%'):\n",
    "            return float(rate_str.strip('%')) / 100\n",
    "    except ValueError:\n",
    "        # In case there is an error with the conversion, return None or handle it appropriately\n",
    "        return None\n",
    "    \n",
    "    # If the rate is not a percentage and not \"N/A\", return it as it is\n",
    "    # Or you might want to handle this case differently\n",
    "    return rate_str\n",
    "\n",
    "\n",
    "def process_boolean(boolean_str):\n",
    "    \n",
    "    boolean_str = boolean_str.strip().lower()\n",
    "    if boolean_str == \"\":\n",
    "        return None\n",
    "    \n",
    "    if boolean_str == \"t\":\n",
    "        return True\n",
    "    elif boolean_str == \"f\":\n",
    "        return False\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def process_datetime(datetime_str):\n",
    "    \n",
    "    datetime_str = datetime_str.strip()\n",
    "    if datetime_str == \"\":\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return datetime_str\n",
    "    \n",
    "def process_numeric(numeric_str):\n",
    "    numeric_str = numeric_str.strip()\n",
    "    if numeric_str == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        return int(numeric_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "def process_double(value):\n",
    "    # If the value is None or an empty string, return None\n",
    "    if value is None or value == '':\n",
    "        return None\n",
    "    \n",
    "    # If the value is already a float, return it as is\n",
    "    if isinstance(value, float):\n",
    "        return value\n",
    "\n",
    "    # If the value is a string, strip it and convert to float\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return float(value.strip())\n",
    "        except ValueError:\n",
    "            # If conversion fails, return None\n",
    "            return None\n",
    "\n",
    "    # If the value is some other type, handle appropriately or raise an error\n",
    "    raise TypeError(f\"Value '{value}' is not a string or float and cannot be processed as a double precision number.\")\n",
    "\n",
    "    \n",
    "\n",
    "def process_currency(currency_str):\n",
    "    \n",
    "    if currency_str is None: return None\n",
    "    try:\n",
    "        amount = float(re.sub(r\"[^\\d.]\", \"\", currency_str))\n",
    "        return amount\n",
    "    \n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "print(\"Functions successfully declared ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5511ef-b6cf-4f81-9974-494905df8630",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records of neighbourhoods inserted ...\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file - Loading CSV file - Populating the Neighborhood database\n",
    "import csv\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "def import_neighborhoods_from_csv(csv_file_path, state):\n",
    "   \n",
    "    # Open the CSV file\n",
    "    with open(csv_file_path, newline='') as csvfile:\n",
    "        # Create a CSV reader object using the file object (csvfile)\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        next(csv_reader)  # Skip the header row\n",
    "        \n",
    "        # Prepare the INSERT statement SQL command\n",
    "        insert_statement = \"INSERT INTO Neighborhoods (Neighborhood_group, Neighborhood, State) VALUES (%s, %s, %s)\"\n",
    "        \n",
    "        # Iterate over the rows in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Extract the neighborhood_group and neighborhood from the current row\n",
    "            neighborhood_group = row[0]\n",
    "            neighborhood = row[1]\n",
    "        \n",
    "            \n",
    "            # Execute the INSERT statement\n",
    "            cur.execute(insert_statement, (neighborhood_group, neighborhood, state))\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    \n",
    "csv_file_path = 'usa-2/New York City/neighbourhoods.csv'\n",
    "import_neighborhoods_from_csv(csv_file_path, \"NY\")\n",
    "\n",
    "csv_file_path2 = 'usa-2/Hawaii/neighbourhoods.csv'\n",
    "import_neighborhoods_from_csv(csv_file_path2, \"HI\")\n",
    "\n",
    "print(\"Records of neighbourhoods inserted ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "256ca812-9139-47f5-aea5-4075707ebb08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records inserted ...\n"
     ]
    }
   ],
   "source": [
    "#Hosts\n",
    "import csv\n",
    "import psycopg2\n",
    "\n",
    "def fetch_host_ids(cur):\n",
    "    query = \"SELECT id FROM hosts;\"\n",
    "    cur.execute(query)\n",
    "    results = cur.fetchall()\n",
    "    return set(row[0] for row in results)\n",
    "\n",
    "\n",
    "def import_hosts_from_csv(csv_file_path):\n",
    "  \n",
    "    \n",
    "    # Open the CSV file\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        # Create a CSV reader object using the file object (csvfile)\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Prepare the INSERT statement SQL command\n",
    "        # Make sure the column names match those in your 'CREATE TABLE' statement\n",
    "        insert_statement = \"\"\"\n",
    "        INSERT INTO Hosts (\n",
    "            Id, Host_url, Name, Since, Location, About,\n",
    "            Response_time, Response_rate, Acceptance_rate,\n",
    "            Is_superhost, Neighborhood, Listings_count,\n",
    "            Total_listings_count, Verifications,\n",
    "            Has_profile_pic, Identity_verified\n",
    "        ) VALUES (\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Iterate over the rows in the CSV file\n",
    "        hosts_by_id = {}\n",
    "        existing_host_ids = fetch_host_ids(cur)\n",
    "          \n",
    "        for row in csv_reader:\n",
    "        # Use the Id as the key\n",
    "            key = int(row['host_id'])\n",
    "            if key in existing_host_ids:\n",
    "                continue\n",
    "            if key not in hosts_by_id:\n",
    "                hosts_by_id[key] = []\n",
    "            hosts_by_id[key].append(row)\n",
    "            \n",
    "        for host_id, rows in hosts_by_id.items():\n",
    "            # Execute the INSERT statement\n",
    "            row=rows[0]\n",
    "            host_acceptance_rate = process_rate(row['host_acceptance_rate'])\n",
    "            host_response_rate = process_rate(row['host_response_rate'])\n",
    "            \n",
    "            cur.execute(insert_statement, (\n",
    "                row['host_id'], row['host_url'], row['host_name'], process_datetime(row['host_since']),\n",
    "                row['host_location'], row['host_about'], row['host_response_time'], \n",
    "                process_double(host_response_rate), process_double(host_acceptance_rate), process_boolean(row['host_is_superhost']), \n",
    "                row['host_neighbourhood'], process_numeric(row['host_listings_count']), process_numeric(row['host_total_listings_count']), \n",
    "                row['host_verifications'], process_boolean(row['host_has_profile_pic']), process_boolean(row['host_identity_verified'])\n",
    "            ))\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "    \n",
    "\n",
    "csv_file_path = 'usa-2/New York City/listings_detailed.csv'\n",
    "import_hosts_from_csv(csv_file_path)\n",
    "\n",
    "csv_file_path2 = 'usa-2/Hawaii/listings_detailed.csv'\n",
    "import_hosts_from_csv(csv_file_path2)\n",
    "\n",
    "print(\"Records inserted ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667499f5-379a-47d5-9575-e374d4436612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records inserted ...\n"
     ]
    }
   ],
   "source": [
    "#Listings table \n",
    "\n",
    "def fetch_neighborhood_ids(cur):\n",
    "    query = \"SELECT id, neighborhood_group, neighborhood FROM neighborhoods;\"\n",
    "    cur.execute(query)\n",
    "    results = cur.fetchall()\n",
    "    \n",
    "    neighborhood_dict = {}\n",
    "    for (Id, neighborhood_group, neighborhood) in results:\n",
    "        # The key is a tuple of (neighborhood_group, neighborhood)\n",
    "        key = (neighborhood_group, neighborhood)\n",
    "        neighborhood_dict[key] = Id\n",
    "        \n",
    "    return neighborhood_dict\n",
    "\n",
    "\n",
    "\n",
    "def import_listings_from_csv(csv_file_path):\n",
    "    \n",
    "    neighborhood_ids = fetch_neighborhood_ids(cur)\n",
    "    \n",
    "    # Open the CSV file\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        # Create a CSV reader object using the file object (csvfile)\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Prepare the INSERT statement SQL command\n",
    "        insert_statement = \"\"\"\n",
    "        INSERT INTO Listings (\n",
    "            Id, Name, Host_id, Neighborhood_id, Description, Neighborhood_overview, Picture_url,\n",
    "            Latitude, Longitude, Property_type, Accommodates, Bathrooms_text,\n",
    "            Beds, Bedrooms, Amenities, Price, Minimum_nights, Maximum_nights\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        for row in csv_reader:\n",
    "            # Extract the neighborhood_group and neighborhood from the row\n",
    "            neighborhood_group = row.get('neighbourhood_group_cleansed')\n",
    "            neighborhood = row.get('neighbourhood_cleansed')\n",
    "            \n",
    "            # Use the extracted values to find the corresponding Neighborhood_id\n",
    "            neighborhood_key = (neighborhood_group, neighborhood)\n",
    "            neighborhood_id = neighborhood_ids.get(neighborhood_key)\n",
    "            \n",
    "            if neighborhood_id is None:\n",
    "                print(f\"Neighborhood ID not found for {neighborhood_key}\")\n",
    "                continue  # Skip this row or handle it appropriately\n",
    "\n",
    "        \n",
    "            \n",
    "            # Here we are assuming the CSV column names match the database field names exactly.\n",
    "            # You'll need to replace 'column_name' with the actual column names from your CSV.\n",
    "            cur.execute(insert_statement, (\n",
    "                row['id'],row['name'], row['host_id'], neighborhood_id, row['description'],\n",
    "                row['neighborhood_overview'], row['picture_url'], process_double(row['latitude']),\n",
    "                process_double(row['longitude']), row['property_type'], process_numeric(row['accommodates']),\n",
    "                row['bathrooms_text'], process_numeric(row['beds']), process_numeric(row['bedrooms']),\n",
    "                row['amenities'], process_currency(row['price']), process_numeric(row['minimum_nights']), process_numeric(row['maximum_nights'])\n",
    "            ))\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "csv_file_path = 'usa-2/New York City/listings_detailed.csv'\n",
    "import_listings_from_csv(csv_file_path)\n",
    "\n",
    "csv_file_path2 = 'usa-2/Hawaii/listings_detailed.csv'\n",
    "import_listings_from_csv(csv_file_path2)\n",
    "\n",
    "print(\"Records inserted ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1eb9cfd-1426-4c55-bf44-15ae06d41456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed temp file: <_io.TextIOWrapper name='temp_calendar_data.csv' mode='w' encoding='cp1252'>\n",
      "Total rows processed: 15664741\n",
      "processed temp file: <_io.TextIOWrapper name='temp_calendar_data.csv' mode='w' encoding='cp1252'>\n",
      "Total rows processed: 11227079\n",
      "Data loaded ...\n"
     ]
    }
   ],
   "source": [
    "#Calendar\n",
    "import csv\n",
    "import psycopg2\n",
    "\n",
    "def fetch_listing_ids(cur):\n",
    "    query = \"SELECT id FROM listings;\"\n",
    "    cur.execute(query)\n",
    "    results = cur.fetchall()\n",
    "    return set(row[0] for row in results)\n",
    "        \n",
    "\n",
    "def import_calendar_from_csv(csv_file_path):\n",
    "    listing_ids = fetch_listing_ids(cur)\n",
    "    error_listing_ids = set()\n",
    "    total = 0\n",
    "    temp_file_path = 'temp_calendar_data.csv'  # Temporary file for processed data\n",
    "\n",
    "    # Open the input CSV file and a temporary file for writing processed data\n",
    "    with open(csv_file_path, newline='') as csvfile, open(temp_file_path, 'w', newline='') as temp_file:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        writer = csv.writer(temp_file)\n",
    "\n",
    "        next(csv_reader)  # Skip the header row\n",
    "\n",
    "        # Iterate over the rows in the input CSV file\n",
    "        for row in csv_reader:\n",
    "            total += 1\n",
    "            listing_id = int(row[0])\n",
    "            # Check if the listing ID is valid; if not, skip the row\n",
    "            \n",
    "            if listing_id not in listing_ids:\n",
    "                error_listing_ids.add(listing_id)\n",
    "                continue\n",
    "            date = row[1]\n",
    "            available = 'TRUE' if row[2].lower() in ['true', '1', 't'] else 'FALSE'\n",
    "            price = process_currency(row[3])\n",
    "            adjusted_price = process_currency(row[4])\n",
    "\n",
    "            # Write the processed row to the temporary file\n",
    "            writer.writerow([listing_id, date, available, price, adjusted_price])\n",
    "    print(\"processed temp file:\", temp_file)\n",
    "    # Use COPY command to load data from the temporary file into the database\n",
    "    with open(temp_file_path, 'r') as temp_file:\n",
    "        cur.copy_expert(\"COPY calendar (listing_id, date, available, price, adjusted_price) FROM STDIN WITH CSV\", temp_file)\n",
    "\n",
    "    conn.commit()\n",
    "    \n",
    "    if len(error_listing_ids) > 0:\n",
    "        print(f\"Warning: {len(error_listing_ids)} listing_ids not found in listing\")\n",
    "        \n",
    "    print(f\"Total rows processed: {total}\")\n",
    "\n",
    "# Usage example\n",
    "csv_file_path = 'usa-2/New York City/calendar.csv'\n",
    "import_calendar_from_csv(csv_file_path)\n",
    "\n",
    "csv_file_path2 = 'usa-2/Hawaii/calendar.csv'\n",
    "import_calendar_from_csv(csv_file_path2)\n",
    "\n",
    "print(\"Data loaded ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b1657b7-5575-49af-b607-71b60cc3e4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded ...\n"
     ]
    }
   ],
   "source": [
    "#Reviews_detailed\n",
    "\n",
    "def import_reviews_from_csv(csv_file_path):\n",
    "    listing_ids = fetch_listing_ids(cur)\n",
    "    error_listing_ids = set()\n",
    "    temp_file_path = 'temp_reviews_data.csv'  # Temporary file for processed data\n",
    "    \n",
    "    #Open the input CSV file and a temporary file for writing processed data\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile, open(temp_file_path, 'w', newline='', encoding='utf-8') as temp_file:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        writer = csv.writer(temp_file)\n",
    "\n",
    "        next(csv_reader)  # Skip the header row\n",
    "\n",
    "        \n",
    "        # Iterate over the rows in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Extract the information from the current row\n",
    "            listing_id = int(row[0])\n",
    "            review_id = row[1]\n",
    "            if listing_id not in listing_ids:\n",
    "                error_listing_ids.add(listing_id)\n",
    "                continue\n",
    "            date = row[2]\n",
    "            reviewer_id = row[3]\n",
    "            reviewer_name = row[4]\n",
    "            comments = row[5]\n",
    "            \n",
    "            # Write the processed row to the temporary file\n",
    "            writer.writerow([review_id, listing_id, date, reviewer_id, reviewer_name, comments])\n",
    "            \n",
    "    with open(temp_file_path, 'r', encoding='utf-8') as temp_file:\n",
    "        cur.copy_expert(\"COPY reviews (id, listing_id, date, reviewer_id, reviewer_name, comments) FROM STDIN WITH CSV\", temp_file)\n",
    "\n",
    "    \n",
    "    \n",
    "    if len(error_listing_ids) > 0:\n",
    "        print(f\"Warning: {len(error_listing_ids)} listing_ids not found in listing\")\n",
    "        \n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "    \n",
    "csv_file_path = 'usa-2/New York City/reviews_detailed.csv'\n",
    "import_reviews_from_csv(csv_file_path)\n",
    "\n",
    "csv_file_path2 = 'usa-2/Hawaii/reviews_detailed.csv'\n",
    "import_reviews_from_csv(csv_file_path2)\n",
    "\n",
    "print(\"Data loaded ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12ea2bc3-0661-4dac-ab11-814263b5c3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records have been rolled back\n"
     ]
    }
   ],
   "source": [
    "#IN CASE THERE IS AN ERROR \"INSQLTRANSACTION\"\n",
    "conn.rollback()\n",
    "\n",
    "print(\"Records have been rolled back ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5779f42d-cf7a-401b-92c3-e74fc53c9245",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    " # Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Connection closed ...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
